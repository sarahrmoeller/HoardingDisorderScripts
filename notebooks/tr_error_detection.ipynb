{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "42bf09be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Input folder: /Users/admin/Documents/coding_land/HoardingDisorderScripts/data/s1055-1058/REVIEW\n",
      "[INFO] Found 37 files matching *.txt.json\n",
      "[INFO] After filtering by stem '055_684': 1 file(s)\n",
      "\n",
      "======================================================================\n",
      "[FILE] 055_684.txt.json\n",
      "All Named Entities (NER):\n",
      "26:28                          -> CARDINAL\n",
      "26:42                          -> CARDINAL\n",
      "26:48                          -> CARDINAL\n",
      "I’ve                           -> PERSON\n",
      "27:01                          -> CARDINAL\n",
      "27:07                          -> CARDINAL\n",
      "Gotcha                         -> PERSON\n",
      "two                            -> CARDINAL\n",
      "27:33                          -> CARDINAL\n",
      "27:38                          -> CARDINAL\n",
      "27:41                          -> CARDINAL\n",
      "28:09                          -> CARDINAL\n",
      "28:18                          -> CARDINAL\n",
      "Gotcha                         -> PERSON\n",
      "28:42                          -> CARDINAL\n",
      "29:04                          -> CARDINAL\n",
      "29:12                          -> CARDINAL\n",
      "29:18                          -> CARDINAL\n",
      "Gotcha                         -> PERSON\n",
      "29:55                          -> CARDINAL\n",
      "30:02                          -> CARDINAL\n",
      "30:12                          -> CARDINAL\n",
      "Mhmm                           -> PERSON\n",
      "30:26                          -> CARDINAL\n",
      "\n",
      "PERSON entities:\n",
      "I’ve\n",
      "Gotcha\n",
      "Gotcha\n",
      "Gotcha\n",
      "Mhmm\n",
      "\n",
      "Title + Name pairs (POS-based):\n",
      "(none)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import stanza\n",
    "import re\n",
    "from difflib import get_close_matches\n",
    "import sys \n",
    "\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "from utils.regexes import SPEAKER_PAIRS, SPEAKERS, speaker_labels, speaker_labels_restricted\n",
    "\n",
    "nlp = stanza.Pipeline(lang=\"en\", processors=\"tokenize,mwt,pos,lemma,ner\", verbose=False)\n",
    "\n",
    "class ErrorDetector:\n",
    "    \"\"\"\n",
    "    Stanza detection only:\n",
    "      - Print PERSON entities (default)\n",
    "      - Optionally print ALL entities\n",
    "      - Optionally print simple Title+Name pairs (POS-based heuristic)\n",
    "    \"\"\"\n",
    "    def __init__(self, text: str, nlp_pipeline):\n",
    "        self.text = text\n",
    "        self.doc = nlp_pipeline(text)\n",
    "\n",
    "    def detect_proper_nouns(self, include_non_person: bool = False, show_title_pairs: bool = True):\n",
    "        # show all NER entities\n",
    "        if include_non_person:\n",
    "            print(\"All Named Entities (NER):\")\n",
    "            for ent in self.doc.ents:\n",
    "                print(f\"{ent.text:<30} -> {ent.type}\")\n",
    "\n",
    "        # PERSON-only view\n",
    "        print(\"\\nPERSON entities:\")\n",
    "        person_tags = {\"PERSON\", \"PER\"}\n",
    "        found_person = False\n",
    "        for ent in self.doc.ents:\n",
    "            if ent.type in person_tags:\n",
    "                found_person = True\n",
    "                print(ent.text)\n",
    "        if not found_person:\n",
    "            print(\"(none)\")\n",
    "\n",
    "        # Lightweight title + name using POS\n",
    "        if show_title_pairs:\n",
    "            print(\"\\nTitle + Name pairs (POS-based):\")\n",
    "            title_words = {\"Dr.\", \"Mr.\", \"Ms.\", \"Mrs.\", \"Prof.\", \"Professor\"}\n",
    "            found_pair = False\n",
    "            for sent in self.doc.sentences:\n",
    "                words = sent.words\n",
    "                for i in range(len(words) - 1):\n",
    "                    if words[i].text in title_words and words[i + 1].upos == \"PROPN\":\n",
    "                        found_pair = True\n",
    "                        print(f\"{words[i].text} {words[i+1].text}\")\n",
    "            if not found_pair:\n",
    "                print(\"(none)\")\n",
    "\n",
    "    def process_transcripts(input_folder, only_stem: str | None = None,\n",
    "                            include_non_person: bool = False, show_title_pairs: bool = True):\n",
    "        # Resolve and list files\n",
    "        input_path = Path(input_folder).resolve()\n",
    "        print(f\"[INFO] Input folder: {input_path}\")\n",
    "        files = sorted(input_path.glob(\"*.txt.json\"))\n",
    "        print(f\"[INFO] Found {len(files)} files matching *.txt.json\")\n",
    "        if only_stem:\n",
    "            files = [f for f in files if f.stem.startswith(only_stem)]\n",
    "            print(f\"[INFO] After filtering by stem '{only_stem}': {len(files)} file(s)\")\n",
    "\n",
    "        if not files:\n",
    "            print(\"[WARN] No matching files.\")\n",
    "            return\n",
    "\n",
    "        # Loop files and print detections\n",
    "        for file in files:\n",
    "            print(\"\\n\" + \"=\" * 70)\n",
    "            print(f\"[FILE] {file.name}\")\n",
    "            with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "            rows = data.get(\"rows\", [])\n",
    "            if not rows:\n",
    "                print(\"[WARN] Skipping: No 'rows' key or it's empty.\")\n",
    "                continue\n",
    "\n",
    "            text = \"\\n\".join(row.get(\"content\", \"\") for row in rows)\n",
    "            det = ErrorDetector(text, nlp)\n",
    "            det.detect_proper_nouns(include_non_person=include_non_person, show_title_pairs=show_title_pairs)\n",
    "\n",
    "    def process_string(sample_text: str, include_non_person: bool = True, show_title_pairs: bool = True):\n",
    "        print(\"[INFO] Running detection on a literal string…\")\n",
    "        det = ErrorDetector(sample_text, nlp)\n",
    "        det.detect_proper_nouns(include_non_person=include_non_person, show_title_pairs=show_title_pairs)\n",
    "\n",
    "    if __name__ == \"__main__\":\n",
    "        # Example: adjust to your project layout\n",
    "        project_root = Path.cwd().parent\n",
    "        input_folder = project_root / \"data\" / \"s1055-1058\" / \"REVIEW\"\n",
    "\n",
    "        # Target just one file (two test files. in use are 057_707 and 055_684)\n",
    "        process_transcripts(\n",
    "            input_folder=input_folder,\n",
    "            only_stem=\"055_684\",          # set to None to scan all\n",
    "            include_non_person=True,       # set False to show only PERSON entities\n",
    "            show_title_pairs=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ec74f16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(str(Path.cwd().parent))\n",
    "from utils.regexes import SPEAKER_PAIRS, SPEAKERS, speaker_labels, speaker_labels_restricted\n",
    "\n",
    "def find_speaker_format_issues(text, speaker_set):\n",
    "        \"\"\"\n",
    "       Method 2.1 Detects speaker label formatting issues:\n",
    "            1. Speaker label followed by space before colon (e.g., 'Participant :')\n",
    "            2. Speaker label followed by punctuation or character other than ':' (e.g., 'Participant.', 'Participant-')\n",
    "        Returns a dictionary with issue types and matching instances.\n",
    "        \"\"\"\n",
    "        issues = {}\n",
    "\n",
    "        # Pattern 1: Space before colon \"Participant :\" (colon spacing issue)\n",
    "        spacing_pattern = re.compile(r'\\b(?:' + '|'.join(re.escape(s) for s in speaker_set) + r')\\s+:')\n",
    "        spacing_matches = spacing_pattern.findall(text)\n",
    "        if spacing_matches:\n",
    "            issues['spacing_issue'] = spacing_matches\n",
    "\n",
    "        # Pattern 2: Speaker label followed by something other than colon or space (e.g., 'Participant.' or 'Participant!')\n",
    "        bad_punct_pattern = re.compile(r'\\b(?:' + '|'.join(re.escape(s) for s in speaker_set) + r')[^\\s:]')\n",
    "        punct_matches = bad_punct_pattern.findall(text)\n",
    "        if punct_matches:\n",
    "            issues['bad_punctuation'] = punct_matches\n",
    "\n",
    "        return issues\n",
    "\n",
    "def find_spelling_variants(text, speaker_set, threshold=0.8):\n",
    "    '''\n",
    "    Method 3.1 Finds likely misspellings of speaker labels using fuzzy matching.\n",
    "    '''\n",
    "    pattern = re.compile(r'^([A-Z][a-zA-Z0-9_ ]{1,30})(?=\\s*:\\s*)', re.MULTILINE)\n",
    "    candidates = pattern.findall(text)\n",
    "\n",
    "    fuzzy_hits = {}\n",
    "    for cand in candidates:\n",
    "        matches = get_close_matches(cand, speaker_set, n=1, cutoff=threshold)\n",
    "        if matches and matches[0] != cand:\n",
    "            fuzzy_hits[cand] = matches[0]\n",
    "    return fuzzy_hits\n",
    "\n",
    "def find_multi_speaker_lines(text):\n",
    "    '''\n",
    "    Method 3.2 Finds lines that contain more than one speaker label.\n",
    "    '''\n",
    "    speaker_pattern = r'\\b(?:' + '|'.join(re.escape(s) for s in SPEAKERS) + r')\\s*:'\n",
    "    pattern = re.compile(speaker_pattern)\n",
    "    multi_speaker_lines = []\n",
    "    for i, line in enumerate(text.splitlines()):\n",
    "        matches = pattern.findall(line)\n",
    "        if len(matches) > 1:\n",
    "            multi_speaker_lines.append((i + 1, line.strip(), matches))\n",
    "    return multi_speaker_lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6d9d9565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] CWD:        /Users/admin/Documents/coding_land/HoardingDisorderScripts/notebooks\n",
      "[INFO] Project:    /Users/admin/Documents/coding_land/HoardingDisorderScripts\n",
      "[INFO] Review dir: /Users/admin/Documents/coding_land/HoardingDisorderScripts/data/s1055-1058/REVIEW\n",
      "[INFO] File:       /Users/admin/Documents/coding_land/HoardingDisorderScripts/data/s1055-1058/REVIEW/057_707.txt.json\n",
      "[INFO] Built text length: 488 chars\n",
      "\n",
      "NO ERRORS\n"
     ]
    }
   ],
   "source": [
    "#from pathlib import Path\n",
    "#import json\n",
    "\n",
    "# ---- config ----\n",
    "FILENAME = \"057_707.txt.json\"   # change if needed\n",
    "\n",
    "# In a notebook, CWD is usually .../HoardingDisorderScripts/notebooks\n",
    "project_root = Path.cwd().parent\n",
    "review_dir = project_root / \"data\" / \"s1055-1058\" / \"REVIEW\"\n",
    "\n",
    "print(f\"[INFO] CWD:        {Path.cwd()}\")\n",
    "print(f\"[INFO] Project:    {project_root}\")\n",
    "print(f\"[INFO] Review dir: {review_dir}\")\n",
    "\n",
    "if not review_dir.exists():\n",
    "    raise FileNotFoundError(\"Review dir not found. Check the path to data/s1055-1058/REVIEW.\")\n",
    "\n",
    "file_path = (review_dir / FILENAME).resolve()\n",
    "print(f\"[INFO] File:       {file_path}\")\n",
    "\n",
    "if not file_path.exists():\n",
    "    # helpful listing so you can pick the right name\n",
    "    print(\"[WARN] File not found. Here are some candidates in REVIEW/:\")\n",
    "    for p in sorted(review_dir.glob(\"*.txt.json\"))[:20]:\n",
    "        print(\"  -\", p.name)\n",
    "    raise FileNotFoundError(f\"{FILENAME} not found in {review_dir}\")\n",
    "\n",
    "# ---- load JSON ----\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# ---- build text ----\n",
    "text = \"\"\n",
    "if isinstance(data.get(\"full_content\"), str) and data[\"full_content\"].strip():\n",
    "    text = data[\"full_content\"]\n",
    "elif isinstance(data.get(\"rows\"), list) and data[\"rows\"]:\n",
    "    text = \"\\n\".join(row.get(\"content\", \"\") for row in data[\"rows\"] if isinstance(row.get(\"content\"), str))\n",
    "elif isinstance(data.get(\"text\"), str) and data[\"text\"].strip():\n",
    "    text = data[\"text\"]\n",
    "\n",
    "print(f\"[INFO] Built text length: {len(text):,} chars\")\n",
    "if not text.strip():\n",
    "    raise ValueError(\"Couldn't build text from 'full_content', 'rows', or 'text'.\")\n",
    "\n",
    "# ---- run YOUR existing detectors (they must already be defined/imported) ----\n",
    "speaker_set = set(SPEAKERS)\n",
    "\n",
    "fmt_issues   = find_speaker_format_issues(text, speaker_set)\n",
    "misspellings = find_spelling_variants(text, speaker_set, threshold=0.8)\n",
    "multi_lines  = find_multi_speaker_lines(text)\n",
    "\n",
    "total = sum(len(v) for v in fmt_issues.values()) + len(misspellings) + len(multi_lines)\n",
    "\n",
    "# ---- print summary ----\n",
    "if total:\n",
    "    print(f\"\\nYES — errors found: {total}\")\n",
    "\n",
    "    if fmt_issues.get(\"spacing_issue\"):\n",
    "        print(f\"  • Space-before-colon: {len(fmt_issues['spacing_issue'])}\")\n",
    "        for ln, snip, _ in fmt_issues[\"spacing_issue\"][:10]:\n",
    "            print(f\"      line {ln}: {snip!r}\")\n",
    "\n",
    "    if fmt_issues.get(\"bad_punctuation\"):\n",
    "        print(f\"  • Bad punctuation after label: {len(fmt_issues['bad_punctuation'])}\")\n",
    "        for ln, snip, _ in fmt_issues[\"bad_punctuation\"][:10]:\n",
    "            print(f\"      line {ln}: {snip!r}\")\n",
    "\n",
    "    if misspellings:\n",
    "        print(f\"  • Likely misspellings: {len(misspellings)}\")\n",
    "        for bad, sug in list(misspellings.items())[:10]:\n",
    "            print(f\"      {bad!r} -> {sug!r}\")\n",
    "\n",
    "    if multi_lines:\n",
    "        print(f\"  • Multiple labels on one line: {len(multi_lines)}\")\n",
    "        for ln, line_txt, labels in multi_lines[:5]:\n",
    "            print(f\"      line {ln}: labels={labels} | {line_txt[:120]}\")\n",
    "else:\n",
    "    print(\"\\nNO ERRORS\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "628f0c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "YES — errors found: 9\n",
      "  • Space-before-colon: 1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fmt_issues\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspacing_issue\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  • Space-before-colon: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(fmt_issues[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspacing_issue\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 34\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ln, snip, full \u001b[38;5;129;01min\u001b[39;00m fmt_issues[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspacing_issue\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m     35\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m      line \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mln\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msnip\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fmt_issues\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbad_punctuation\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "# Test transcript with all target errors \n",
    "test_text = \"\"\"\\\n",
    "Interviewer: Hi, thanks for joining us.                        # valid\n",
    "Participant : I'm good, thanks.                                # spacing-before-colon (bad)\n",
    "Interveiwer: Can you tell me your name?                        # misspelling (bad)\n",
    "Interviewee. Please describe your symptoms.                    # bad punctuation after label (.)\n",
    "Speaker- What brings you here today?                           # bad punctuation after label (-)\n",
    "Interviewr : This has both spelling and spacing issues.        # misspelling + spacing-before-colon (bad)\n",
    "Interviewer: What's your favorite food? Participant: Pizza.    # multiple labels on one line (bad)\n",
    "Participant's favorite color is blue.                          # label immediately followed by apostrophe (bad)\n",
    "Interviewee: All good here.                                    # valid\n",
    "Interviewee? Could you repeat that?                            # bad punctuation after label (?)\n",
    "\"\"\"\n",
    "\n",
    "# Use your project’s canonical speakers if available:\n",
    "try:\n",
    "    speaker_set = set(SPEAKERS)  # from utils.regexes\n",
    "except NameError:\n",
    "    speaker_set = {\"Interviewer\", \"Participant\", \"Interviewee\", \"Speaker\"}\n",
    "\n",
    "# --- Run your existing detectors ---\n",
    "fmt_issues   = find_speaker_format_issues(test_text, speaker_set)\n",
    "misspellings = find_spelling_variants(test_text, speaker_set, threshold=0.8)  # or 0.85 if you want stricter\n",
    "multi_lines  = find_multi_speaker_lines(test_text)\n",
    "\n",
    "total = sum(len(v) for v in fmt_issues.values()) + len(misspellings) + len(multi_lines)\n",
    "\n",
    "# Print summary\n",
    "if total:\n",
    "    print(f\"\\nYES — errors found: {total}\")\n",
    "\n",
    "    if fmt_issues.get(\"spacing_issue\"):\n",
    "        print(f\"  • Space-before-colon: {len(fmt_issues['spacing_issue'])}\")\n",
    "        for ln, snip, full in fmt_issues[\"spacing_issue\"]:\n",
    "            print(f\"      line {ln}: {snip!r}\")\n",
    "\n",
    "    if fmt_issues.get(\"bad_punctuation\"):\n",
    "        print(f\"  • Bad punctuation after label: {len(fmt_issues['bad_punctuation'])}\")\n",
    "        for ln, snip, full in fmt_issues[\"bad_punctuation\"]:\n",
    "            print(f\"      line {ln}: {snip!r}\")\n",
    "\n",
    "    if misspellings:\n",
    "        print(f\"  • Likely misspellings: {len(misspellings)}\")\n",
    "        for bad, sug in misspellings.items():\n",
    "            print(f\"      {bad!r} -> {sug!r}\")\n",
    "\n",
    "    if multi_lines:\n",
    "        print(f\"  • Multiple labels on one line: {len(multi_lines)}\")\n",
    "        for ln, line_txt, labels in multi_lines:\n",
    "            print(f\"      line {ln}: labels={labels} | {line_txt[:120]}\")\n",
    "else:\n",
    "    print(\"\\nNO ERRORS\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
