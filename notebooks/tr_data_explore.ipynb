{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf52f7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b885529d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/admin/Documents/coding_land/HoardingDisorderScripts/.venv/lib/python3.11/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /Users/admin/Documents/coding_land/HoardingDisorderScripts/.venv/lib/python3.11/site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in /Users/admin/Documents/coding_land/HoardingDisorderScripts/.venv/lib/python3.11/site-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/admin/Documents/coding_land/HoardingDisorderScripts/.venv/lib/python3.11/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /Users/admin/Documents/coding_land/HoardingDisorderScripts/.venv/lib/python3.11/site-packages (from nltk) (4.67.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5ff406c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/admin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7788351",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Get the project root (parent of notebooks/)\n",
    "project_root = Path.cwd().parent\n",
    "data_dir = project_root / \"data\"\n",
    "\n",
    "for project in os.listdir(data_dir):\n",
    "    dir_path = data_dir / project / \"REVIEW\"\n",
    "    if os.path.isdir(dir_path):\n",
    "        # process directory\n",
    "        pass\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fe5f518",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /Users/admin/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
      "[nltk_data] Downloading package punkt to /Users/admin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "from utils.document import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11bc0198",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "982f61f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.document import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83a173ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "057_707.txt\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "project_root = Path.cwd().parent\n",
    "doc_path = project_root / \"data\" / \"s1055-1058\" / \"REVIEW\" / \"057_707.txt.json\"\n",
    "doc = Document(str(doc_path))\n",
    "print(doc.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50858202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Participant:\n",
      "No, I don‚Äôt think so. I mean, hopefully I still qualify for the $30 gift card, is that-\n",
      "Interviewer:\n",
      "Yeah, you, you participated so I would assume so.\n",
      "Participant:\n",
      "Great.\n",
      "Interviewer:\n",
      "But if you have any questions, feel free to reach out to Dr. Matthews.\n",
      "Participant:\n",
      "Avery? Or, not Avery, Dr. Mathews. Okay. Sounds good.\n",
      "Interviewer:\n",
      "Thank you so much.\n",
      "Participant:\n",
      "Awesome thanks Avery.\n",
      "Interviewer:\n",
      "Have a great day.\n",
      "Participant:\n",
      "Yeah, you too. Bye bye.\n",
      "[END OF RECORDING]\n"
     ]
    }
   ],
   "source": [
    "# Could print dir for all methods\n",
    "print(doc.full_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0a4dba7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FileSystemPathPointer('/Users/admin/nltk_data/tokenizers/punkt')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.data.find('tokenizers/punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd46f90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Document Name: 055_684.txt\n",
      "Transcript Number: 055\n",
      "Set Number: 1\n",
      "Hoarder Flag: 1\n",
      "Project Name: s1055-1058\n",
      "========================================\n",
      "\n",
      "üßæ Full Content Preview:\n",
      "Interviewer (26:28): Okay, so, um, sentimentality seems to be like a pretty big deal in terms of, you know, why you think things are valuable. Would you say that's fair?\n",
      "Participant:\n",
      "Yes.\n",
      "Interviewer (26:42): Alright. So, in your personal opinion, do you think you have too much stuff?\n",
      "Participant:\n",
      "Yes.\n",
      "Interviewer (26:48): Alright, so, how do you know that?\n",
      "Participant:\n",
      "I‚Äôve just got stuff everywhere. Um‚Ä¶\n",
      "Interviewer (27:01): Right, so because it takes up so much space in your house?\n",
      "Participant ...\n",
      "\n",
      "üë• Speaker Set:\n",
      "{'Participant'}\n",
      "\n",
      "üéôÔ∏è Speaker Tuple (Identified Pair):\n",
      "('Interviewer', 'Participant')\n",
      "\n",
      "üìä Label Counts:\n",
      "Unclear‚ÄìInterviewer: 0\n",
      "Unclear‚ÄìParticipant: 1\n",
      "Unclear‚ÄìTotal: 1\n",
      "Overlap‚ÄìInterviewer: 0\n",
      "Overlap‚ÄìParticipant: 2\n",
      "Overlap‚ÄìTotal: 2\n",
      "Misspeak‚ÄìInterviewer: 0\n",
      "Misspeak‚ÄìParticipant: 0\n",
      "Misspeak‚ÄìTotal: 0\n",
      "Clarification‚ÄìInterviewer: 0\n",
      "Clarification‚ÄìParticipant: 0\n",
      "Clarification‚ÄìTotal: 0\n",
      "Generic Disfluency‚ÄìInterviewer: 0\n",
      "Generic Disfluency‚ÄìParticipant: 10\n",
      "Generic Disfluency‚ÄìTotal: 10\n",
      "Self Correction‚ÄìInterviewer: 0\n",
      "Self Correction‚ÄìParticipant: 0\n",
      "Self Correction‚ÄìTotal: 0\n",
      "Incomplete Thought‚ÄìInterviewer: 0\n",
      "Incomplete Thought‚ÄìParticipant: 0\n",
      "Incomplete Thought‚ÄìTotal: 0\n",
      "Total: 13\n",
      "\n",
      "üó£Ô∏è First 5 Lines by Participant (Cleaned):\n",
      "['yes.', 'interviewer : alright. so, in your personal opinion, do you think you have too much stuff?', 'yes.', 'interviewer : alright, so, how do you know that?', 'i‚Äôve just got stuff everywhere. um‚Ä¶']\n",
      "\n",
      "üó£Ô∏è First 5 Lines by Interviewer (Cleaned):\n",
      "[\"interviewer : okay, so, um, sentimentality seems to be like a pretty big deal in terms of, you know, why you think things are valuable. would you say that's fair?\"]\n",
      "\n",
      "üß© First 10 Tokens by Participant:\n",
      "['yes', 'interviewer', 'alright', 'so', 'in', 'your', 'personal', 'opinion', 'do', 'you']\n",
      "\n",
      "üß© First 10 Tokens by Interviewer:\n",
      "['interviewer', 'okay', 'so', 'um', 'sentimentality', 'seems', 'to', 'be', 'like', 'a']\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json  # Required if not already imported\n",
    "from collections import Counter\n",
    "from utils.document import Document  # Replace 'your_module' with your actual module name\n",
    "\n",
    "# Load document\n",
    "project_root = Path.cwd().parent\n",
    "doc_path = project_root / \"data\" / \"s1055-1058\" / \"REVIEW\" / \"055_684.txt.json\"\n",
    "doc = Document(str(doc_path))\n",
    "\n",
    "print(\"=\" * 40)\n",
    "print(f\"Document Name: {doc.name}\")\n",
    "print(f\"Transcript Number: {doc.transcript_number}\")\n",
    "print(f\"Set Number: {doc.set}\")\n",
    "print(f\"Hoarder Flag: {doc.hoarder_flag}\")\n",
    "print(f\"Project Name: {doc.project}\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "print(\"\\nüßæ Full Content Preview:\")\n",
    "print(doc.full_content[:500], \"...\")  # Print only the first 500 characters\n",
    "\n",
    "print(\"\\nüë• Speaker Set:\")\n",
    "print(doc.speaker_set())\n",
    "\n",
    "print(\"\\nüéôÔ∏è Speaker Tuple (Identified Pair):\")\n",
    "print(doc.speaker_tuple)\n",
    "\n",
    "print(\"\\nüìä Label Counts:\")\n",
    "for k, v in doc.label_counts.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "print(\"\\nüó£Ô∏è First 5 Lines by Participant (Cleaned):\")\n",
    "participant_lines = doc.lines_by_speaker(\"Participant\", speaker_labels=False, cleaned=True)\n",
    "print(participant_lines[:5])\n",
    "\n",
    "print(\"\\nüó£Ô∏è First 5 Lines by Interviewer (Cleaned):\")\n",
    "interviewer_lines = doc.lines_by_speaker(\"Interviewer\", speaker_labels=False, cleaned=True)\n",
    "print(interviewer_lines[:5])\n",
    "\n",
    "print(\"\\nüß© First 10 Tokens by Participant:\")\n",
    "participant_tokens = doc.tokens(\"Participant\", flat=True)\n",
    "print(participant_tokens[:10])\n",
    "\n",
    "print(\"\\nüß© First 10 Tokens by Interviewer:\")\n",
    "interviewer_tokens = doc.tokens(\"Interviewer\", flat=True)\n",
    "print(interviewer_tokens[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5fa7df85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 433kB [00:00, 8.37MB/s]                    \n",
      "2025-07-24 17:07:10 INFO: Downloaded file to /Users/admin/stanza_resources/resources.json\n",
      "2025-07-24 17:07:10 INFO: Downloading default packages for language: en (English) ...\n",
      "2025-07-24 17:07:11 INFO: File exists: /Users/admin/stanza_resources/en/default.zip\n",
      "2025-07-24 17:07:13 INFO: Finished downloading models and saved to /Users/admin/stanza_resources\n",
      "2025-07-24 17:07:13 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 433kB [00:00, 29.2MB/s]                    \n",
      "2025-07-24 17:07:13 INFO: Downloaded file to /Users/admin/stanza_resources/resources.json\n",
      "2025-07-24 17:07:13 WARNING: Language en package default expects mwt, which has been added\n",
      "2025-07-24 17:07:13 INFO: Loading these models for language: en (English):\n",
      "=========================================\n",
      "| Processor | Package                   |\n",
      "-----------------------------------------\n",
      "| tokenize  | combined                  |\n",
      "| mwt       | combined                  |\n",
      "| pos       | combined_charlm           |\n",
      "| ner       | ontonotes-ww-multi_charlm |\n",
      "=========================================\n",
      "\n",
      "2025-07-24 17:07:13 INFO: Using device: cpu\n",
      "2025-07-24 17:07:13 INFO: Loading: tokenize\n",
      "2025-07-24 17:07:13 INFO: Loading: mwt\n",
      "2025-07-24 17:07:13 INFO: Loading: pos\n",
      "2025-07-24 17:07:14 INFO: Loading: ner\n",
      "2025-07-24 17:07:16 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# Install stanza if not already installed\n",
    "try:\n",
    "    import stanza\n",
    "except ImportError:\n",
    "    !pip install stanza\n",
    "    import stanza\n",
    "\n",
    "# Download English model (only needs to run once)\n",
    "stanza.download('en')\n",
    "\n",
    "# Load pipeline\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,pos,ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c6d62185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named Entities (NER):\n",
      "26:28                     ‚Üí CARDINAL\n",
      "26:42                     ‚Üí CARDINAL\n",
      "26:48                     ‚Üí CARDINAL\n",
      "I‚Äôve                      ‚Üí PERSON\n",
      "27:01                     ‚Üí CARDINAL\n",
      "27:07                     ‚Üí CARDINAL\n",
      "Gotcha                    ‚Üí PERSON\n",
      "two                       ‚Üí CARDINAL\n",
      "27:33                     ‚Üí CARDINAL\n",
      "27:38                     ‚Üí CARDINAL\n",
      "27:41                     ‚Üí CARDINAL\n",
      "28:09                     ‚Üí CARDINAL\n",
      "28:18                     ‚Üí CARDINAL\n",
      "Gotcha                    ‚Üí PERSON\n",
      "28:42                     ‚Üí CARDINAL\n",
      "29:04                     ‚Üí CARDINAL\n",
      "29:12                     ‚Üí CARDINAL\n",
      "29:18                     ‚Üí CARDINAL\n",
      "Gotcha                    ‚Üí PERSON\n",
      "29:55                     ‚Üí CARDINAL\n",
      "30:02                     ‚Üí CARDINAL\n",
      "30:12                     ‚Üí CARDINAL\n",
      "Mhmm                      ‚Üí PERSON\n",
      "30:26                     ‚Üí CARDINAL\n",
      "\n",
      "Title + Name Pairs (POS-based):\n"
     ]
    }
   ],
   "source": [
    "# Use the full content from your custom Document object\n",
    "text = doc.full_content\n",
    "\n",
    "# Run Stanza and store result as nlp_doc to avoid conflict with original docs\n",
    "nlp_doc = nlp(text)\n",
    "\n",
    "# Named Entity Recognition output\n",
    "print(\"Named Entities (NER):\")\n",
    "for ent in nlp_doc.ents:\n",
    "    print(f\"{ent.text:<25} ‚Üí {ent.type}\")\n",
    "\n",
    "# Title + Name extraction using POS tagging\n",
    "print(\"\\nTitle + Name Pairs (POS-based):\")\n",
    "title_words = {'Dr.', 'Mr.', 'Ms.', 'Mrs.', 'Prof.', 'Professor'}\n",
    "for sentence in nlp_doc.sentences:\n",
    "    words = sentence.words\n",
    "    for i in range(len(words) - 1):\n",
    "        curr_word = words[i]\n",
    "        next_word = words[i + 1]\n",
    "        if curr_word.text in title_words and next_word.upos == 'PROPN':\n",
    "            title_name = f\"{curr_word.text} {next_word.text}\"\n",
    "            print(title_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9225cccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /Users/admin/Documents/coding_land/HoardingDisorderScripts/.venv/lib/python3.11/site-packages (3.8.7)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/admin/Documents/coding_land/HoardingDisorderScripts/.venv/lib/python3.11/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/admin/Documents/coding_land/HoardingDisorderScripts/.venv/lib/python3.11/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/admin/Documents/coding_land/HoardingDisorderScripts/.venv/lib/python3.11/site-packages (from spacy) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/admin/Documents/coding_land/HoardingDisorderScripts/.venv/lib/python3.11/site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/admin/Documents/coding_land/HoardingDisorderScripts/.venv/lib/python3.11/site-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /Users/admin/Documents/coding_land/HoardingDisorderScripts/.venv/lib/python3.11/site-packages (from spacy) (8.3.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/admin/Documents/coding_land/HoardingDisorderScripts/.venv/lib/python3.11/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/admin/Documents/coding_land/HoardingDisorderScripts/.venv/lib/python3.11/site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/admin/Documents/coding_land/HoardingDisorderScripts/.venv/lib/python3.11/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /Users/admin/Documents/coding_land/HoardingDisorderScripts/.venv/lib/python3.11/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /Users/admin/Documents/coding_land/HoardingDisorderScripts/.venv/lib/python3.11/site-packages (from spacy) (0.16.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/admin/Documents/coding_land/HoardingDisorderScripts/.venv/lib/python3.11/site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/admin/Documents/coding_land/HoardingDisorderScripts/.venv/lib/python3.11/site-packages (from spacy) (2.2.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/admin/Documents/coding_land/HoardingDisorderScripts/.venv/lib/python3.11/site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/admin/Documents/coding_land/HoardingDisorderScripts/.venv/lib/python3.11/site-packages (from spacy) (2.11.5)\n",
      "Requirement already satisfied: jinja2 in /Users/admin/Documents/coding_land/HoardingDisorderScripts/.venv/lib/python3.11/site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /Users/admin/Documents/coding_land/HoardingDisorderScripts/.venv/lib/python3.11/site-packages (from spacy) (80.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/admin/Documents/coding_land/HoardingDisorderScripts/.venv/lib/python3.11/site-packages (from spacy) (24.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/admin/Documents/coding_land/HoardingDisorderScripts/.venv/lib/python3.11/site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in /Users/admin/Documents/coding_land/HoardingDisorderScripts/.venv/lib/python3.11/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/admin/Documents/coding_land/HoardingDisorderScripts/.venv/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/admin/Documents/coding_land/HoardingDisorderScripts/.venv/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /Users/admin/Documents/coding_land/HoardingDisorderScripts/.venv/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.13.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/admin/Documents/coding_land/HoardingDisorderScripts/.venv/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/admin/Documents/coding_land/HoardingDisorderScripts/.venv/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/admin/Documents/coding_land/HoardingDisorderScripts/.venv/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/admin/Documents/coding_land/HoardingDisorderScripts/.venv/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/admin/Documents/coding_land/HoardingDisorderScripts/.venv/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.4.26)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /Users/admin/Documents/coding_land/HoardingDisorderScripts/.venv/lib/python3.11/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/admin/Documents/coding_land/HoardingDisorderScripts/.venv/lib/python3.11/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/admin/Documents/coding_land/HoardingDisorderScripts/.venv/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Users/admin/Documents/coding_land/HoardingDisorderScripts/.venv/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /Users/admin/Documents/coding_land/HoardingDisorderScripts/.venv/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy) (14.0.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /Users/admin/Documents/coding_land/HoardingDisorderScripts/.venv/lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /Users/admin/Documents/coding_land/HoardingDisorderScripts/.venv/lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
      "Requirement already satisfied: wrapt in /Users/admin/Documents/coding_land/HoardingDisorderScripts/.venv/lib/python3.11/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /Users/admin/Documents/coding_land/HoardingDisorderScripts/.venv/lib/python3.11/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/admin/Documents/coding_land/HoardingDisorderScripts/.venv/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/admin/Documents/coding_land/HoardingDisorderScripts/.venv/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/admin/Documents/coding_land/HoardingDisorderScripts/.venv/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/admin/Documents/coding_land/HoardingDisorderScripts/.venv/lib/python3.11/site-packages (from jinja2->spacy) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ba99179d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m67.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "!python3 -m spacy download en_core_web_sm\n",
    "\n",
    "nlp_spacy = spacy.load(\"en_core_web_sm\")  # Load spaCy NER model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b539f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading NLP models...\n",
      "Reading file: 057_707.txt.json\n",
      "Transcript preview: Participant:\n",
      "No, I don‚Äôt think so. I mean, hopefully I still qualify for the $30 gift card, is that-\n",
      "Interviewer:\n",
      "Yeah, you, you participated so I would assume so.\n",
      "Participant:\n",
      "Great.\n",
      "Interviewer:\n",
      "But\n",
      "\n",
      "--- DETECTING PROPER NOUNS ---\n",
      "\n",
      "Named Entities (NER):\n",
      "Matthews                  ‚Üí PERSON ‚úÖ\n",
      "Mathews                   ‚Üí PERSON ‚úÖ\n",
      "\n",
      "--- RUNNING NAME REPLACEMENT ---\n",
      "\n",
      "‚ö†Ô∏è ERRORS FOUND (PERSON entities replaced)\n",
      "\n",
      "--- CLEANED TEXT PREVIEW ---\n",
      "Participant:\n",
      "No, I don‚Äôt think so. I mean, hopefully I still qualify for the $30 gift card, is that-\n",
      "Interviewer:\n",
      "Yeah, you, you participated so I would assume so.\n",
      "Participant:\n",
      "Great.\n",
      "Interviewer:\n",
      "But if you have any questions, feel free to reach out to Dr. NAME.\n",
      "Participant:\n",
      "Avery? Or, not Avery, Dr. NAME. Okay. Sounds good.\n",
      "Interviewer:\n",
      "Thank you so much.\n",
      "Participant:\n",
      "Awesome thanks Avery.\n",
      "Interviewer:\n",
      "Have a great day.\n",
      "Participant:\n",
      "Yeah, you too. Bye bye.\n",
      "[END OF RECORDING]\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import stanza\n",
    "import spacy\n",
    "\n",
    "# ---------- Load NLP Pipelines ----------\n",
    "print(\"Loading NLP models...\")\n",
    "stanza_nlp = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos,lemma', verbose=False)\n",
    "nlp_spacy = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "import re\n",
    "# Ignore these labels during NER\n",
    "IGNORE_NAMES = {\"participant\", \"interviewer\"}\n",
    "# Optional: Pre-clean text for NER (removes labels for spaCy but keeps original for masking)\n",
    "def remove_speaker_labels(text: str) -> str:\n",
    "    return re.sub(r'\\b(Participant|Interviewer):', '', text, flags=re.I)\n",
    "\n",
    "\n",
    "\n",
    "# ---------- ErrorDetector Class ----------\n",
    "class ErrorDetector:\n",
    "    def __init__(self, text, stanza_nlp):\n",
    "        self.text = text\n",
    "        self.doc = stanza_nlp(text)        # Stanza for POS, parsing\n",
    "        self.ner_doc = nlp_spacy(text)     # spaCy for NER\n",
    "\n",
    "    def detect_proper_nouns(self):\n",
    "        '''\n",
    "        Detects PERSON entities using spaCy NER,\n",
    "        and filters false positives using Stanza POS.\n",
    "        '''\n",
    "        print(\"\\nNamed Entities (NER):\")\n",
    "        for ent in self.ner_doc.ents:\n",
    "            # Skip ignored names like Participant/Interviewer\n",
    "            if ent.label_ == \"PERSON\" and ent.text.lower().strip(\":\") not in IGNORE_NAMES:\n",
    "                # POS filtering: check if entity contains at least one PROPN in Stanza\n",
    "                ent_start = ent.start_char\n",
    "                ent_end = ent.end_char\n",
    "                is_valid = False\n",
    "                for sentence in self.doc.sentences:\n",
    "                    for word in sentence.words:\n",
    "                        if word.start_char is not None and word.end_char is not None:\n",
    "                            if ent_start <= word.start_char < ent_end:\n",
    "                                if word.upos in { 'PROPN', 'NOUN'}:\n",
    "                                    is_valid = True\n",
    "                                elif word.text.lower() in {'mmhm', 'uh', 'gotcha', 'okay', \"i've\", 'yeah', 'nope'}:\n",
    "                                    is_valid = False\n",
    "                                    break\n",
    "                if is_valid:\n",
    "                    print(f\"{ent.text:<25} ‚Üí PERSON ‚úÖ\")\n",
    "                else:\n",
    "                    print(f\"{ent.text:<25} ‚Üí PERSON ‚ùå (Filtered)\")\n",
    "                \n",
    "        '''# Title + Name extraction using POS tagging (Stanza-only)\n",
    "        print(\"\\nTitle + Name Pairs (POS-based):\")\n",
    "        title_words = {'Dr.', 'Mr.', 'Ms.', 'Mrs.', 'Prof.', 'Professor'}\n",
    "        for sentence in self.doc.sentences:\n",
    "            words = sentence.words\n",
    "            for i in range(len(words) - 1):\n",
    "                curr_word = words[i]\n",
    "                next_word = words[i + 1]\n",
    "                if curr_word.text in title_words and next_word.upos == 'PROPN':\n",
    "                    title_name = f\"{curr_word.text} {next_word.text}\"\n",
    "                    print(title_name)'''\n",
    "\n",
    "    def replace_names(self):\n",
    "        ''' \n",
    "        Replaces PERSON entities with NAME (no filtering here)\n",
    "        '''\n",
    "        masked_text = self.text\n",
    "        offset = 0\n",
    "        was_modified = False\n",
    "\n",
    "        for ent in self.ner_doc.ents:\n",
    "            if ent.label_ == 'PERSON' and ent.text.lower().strip(\":\") not in IGNORE_NAMES:\n",
    "                start = ent.start_char + offset\n",
    "                end = ent.end_char + offset\n",
    "                original = masked_text[start:end]\n",
    "                masked_text = masked_text[:start] + \"NAME\" + masked_text[end:]\n",
    "                offset += len(\"NAME\") - len(original)\n",
    "                was_modified = True\n",
    "\n",
    "        if was_modified:\n",
    "            print(\"\\n‚ö†Ô∏è ERRORS FOUND (PERSON entities replaced)\")\n",
    "        else:\n",
    "            print(\"\\n‚úÖ NO ERRORS (No PERSON entities found)\")\n",
    "\n",
    "        return masked_text\n",
    "\n",
    "\n",
    "# ---------- File Setup ----------\n",
    "test_file = Path.cwd().parent / \"data\" / \"s1055-1058\" / \"REVIEW\" / \"057_707.txt.json\"\n",
    "\n",
    "# ---------- Load File ----------\n",
    "print(f\"Reading file: {test_file.name}\")\n",
    "with open(test_file, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "'''rows = data.get(\"rows\", [])\n",
    "if not rows:\n",
    "    print(\"‚ùå No rows found in the file.\")\n",
    "    exit()'''\n",
    "\n",
    "# ---------- Reconstruct Full Transcript ----------\n",
    "#text = \"\\n\".join(f\"{row['speaker']}: {row['text']}\" for row in rows if 'speaker' in row and 'text' in row)\n",
    "\n",
    "#trying a new constructor:\n",
    "\n",
    "# rows is a list of lists, each inner list has dicts with \"content\"\n",
    "rows = data[\"data\"][\"rows\"]\n",
    "\n",
    "# Flatten and join the contents\n",
    "text = \"\\n\".join(\n",
    "    cell[\"content\"] \n",
    "    for row in rows\n",
    "    for cell in row \n",
    "    if \"content\" in cell and cell[\"content\"].strip()\n",
    ")\n",
    "\n",
    "# Keep original text for replacement, but feed a cleaned version to spaCy\n",
    "text_for_ner = remove_speaker_labels(text)\n",
    "\n",
    "print(\"Transcript preview:\", text[:200])\n",
    "\n",
    "\n",
    "# ---------- Run Detection ----------\n",
    "detector = ErrorDetector(text, stanza_nlp)\n",
    "detector.text_original = text  # keep the original for masking\n",
    "\n",
    "print(\"\\n--- DETECTING PROPER NOUNS ---\")\n",
    "detector.detect_proper_nouns()\n",
    "\n",
    "print(\"\\n--- RUNNING NAME REPLACEMENT ---\")\n",
    "cleaned_text = detector.replace_names()\n",
    "\n",
    "# ---------- Preview Cleaned Output ----------\n",
    "print(\"\\n--- CLEANED TEXT PREVIEW ---\")\n",
    "print(cleaned_text[:500])\n",
    "print(\"\\nDone.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638fac81",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''class ErrorDetector:\n",
    "    def __init__(self, text, stanza_nlp):\n",
    "        self.text = text\n",
    "        self.doc = stanza_nlp(text)        # Stanza for POS, parsing\n",
    "        self.ner_doc = nlp_spacy(text)     # spaCy for NER \n",
    "\n",
    "    def detect_proper_nouns(self):\n",
    "        ''\n",
    "        Detects PERSON entities using spaCy NER,\n",
    "        and filters false positives using Stanza POS. \n",
    "        '''\n",
    "\n",
    "        print(\"Named Entities (NER):\")\n",
    "        for ent in nlp_doc.ents:\n",
    "            if ent.label_ == \"PERSON\":\n",
    "                # POS filtering: check if entity contains at least one PROPN in Stanza\n",
    "                ent_start = ent.start_char\n",
    "                ent_end = ent.end_char\n",
    "                is_valid = False\n",
    "                for sentence in nlp_doc.sentences:\n",
    "                    for word in sentence.words:\n",
    "                        if word.start_char is not None and word.end_char is not None:\n",
    "                            if ent_start <= word.start_char < ent_end:\n",
    "                                if word.upos == 'PROPN':\n",
    "                                    is_valid = True\n",
    "                                elif word.text.lower() in {'mmhm', 'uh', 'gotcha', 'okay', \"i've\", 'yeah', 'nope'}:\n",
    "                                    is_valid = False\n",
    "                                    break\n",
    "                if is_valid:\n",
    "                    print(f\"{ent.text:<25} ‚Üí PERSON ‚úÖ\")\n",
    "                else:\n",
    "                    print(f\"{ent.text:<25} ‚Üí PERSON ‚ùå (Filtered)\")\n",
    "\n",
    "        # Title + Name extraction using POS tagging (Stanza-only)\n",
    "        print(\"\\nTitle + Name Pairs (POS-based):\")\n",
    "        title_words = {'Dr.', 'Mr.', 'Ms.', 'Mrs.', 'Prof.', 'Professor'}\n",
    "        for sentence in nlp_doc.sentences:\n",
    "            words = sentence.words\n",
    "            for i in range(len(words) - 1):\n",
    "                curr_word = words[i]\n",
    "                next_word = words[i + 1]\n",
    "                if curr_word.text in title_words and next_word.upos == 'PROPN':\n",
    "                    title_name = f\"{curr_word.text} {next_word.text}\"\n",
    "                    print(title_name)\n",
    "\n",
    "    def replace_names(self):\n",
    "        ''' \n",
    "        Replaces PERSON entities with NAME (no filtering here)\n",
    "        '''\n",
    "        masked_text = self.text\n",
    "        offset = 0\n",
    "        was_modified = False\n",
    "\n",
    "        for ent in self.ner_doc.ents:\n",
    "            if ent.label_ == 'PERSON':\n",
    "                start = ent.start_char + offset\n",
    "                end = ent.end_char + offset\n",
    "                original = masked_text[start:end]\n",
    "                masked_text = masked_text[:start] + \"NAME\" + masked_text[end:]\n",
    "                offset += len(\"NAME\") - len(original)\n",
    "                was_modified = True\n",
    "\n",
    "        if was_modified:\n",
    "            print(\"ERRORS FOUND\")\n",
    "        else:\n",
    "            print(\"NO ERRORS\")\n",
    "\n",
    "        return masked_text\n",
    "        print(masked_text[:1000])  # Print first 1000 characters for preview\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d95a31cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interviewer (26:28): Okay, so, um, sentimentality seems to be like a pretty big deal in terms of, you know, why you think things are valuable. Would you say that's fair?\n",
      "Participant:\n",
      "Yes.\n",
      "Interviewer (26:42): Alright. So, in your personal opinion, do you think you have too much stuff?\n",
      "Participant:\n",
      "Yes.\n",
      "Interviewer (26:48): Alright, so, how do you know that?\n",
      "Participant:\n",
      "NAME just got stuff everywhere. Um‚Ä¶\n",
      "Interviewer (27:01): Right, so because it takes up so much space in your house?\n",
      "Participant:\n",
      "Yeah.\n",
      "Interviewer (27:07): NAME. So, I know we talked a little bit earlier about, you know, distinguishing between what's trash and what's worth keeping. But, if you were to say, choosing between keeping two items, what kinds of traits would you look at? Or, what kinds of things would you look at in terms of choosing the more important item to keep?\n",
      "Participant:\n",
      "If it‚Äôs something I would use or need.\n",
      "Interviewer (27:33): Okay, usefulness, right?\n",
      "Participant:\n",
      "Yeah.\n",
      "Interviewer (27:38): Okay, an\n"
     ]
    }
   ],
   "source": [
    "# Replace PERSON entities with NAME\n",
    "masked_text = text\n",
    "offset = 0  # Tracks character shifts due to replacement length differences\n",
    "\n",
    "for ent in nlp_doc.ents:\n",
    "    if ent.type == 'PERSON':\n",
    "        start = ent.start_char + offset\n",
    "        end = ent.end_char + offset\n",
    "        original = masked_text[start:end]\n",
    "        masked_text = masked_text[:start] + \"NAME\" + masked_text[end:]\n",
    "        offset += len(\"NAME\") - len(original)\n",
    "\n",
    "print(masked_text[:1000])  # Print first 1000 characters for preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e378bd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import stanza\n",
    "\n",
    "class ErrorDetector:\n",
    "    def __init__(self, text, stanza_nlp, spacy_nlp):\n",
    "        \"\"\"\n",
    "        Initialize ErrorDetector with text, Stanza pipeline, and spaCy pipeline.\n",
    "        \"\"\"\n",
    "        self.text = text\n",
    "        self.doc = stanza_nlp(text)        # Stanza: tokenization, POS, syntax\n",
    "        self.ner_doc = spacy_nlp(text)     # spaCy: Named Entity Recognition\n",
    "\n",
    "    # ----------------------------\n",
    "    # 1. spaCy-based NER\n",
    "    # ----------------------------\n",
    "    def detect_spacy_persons(self):\n",
    "        \"\"\"\n",
    "        Uses spaCy to detect PERSON entities and prints them.\n",
    "        \"\"\"\n",
    "        print(\"=== spaCy PERSON Entities ===\")\n",
    "        for ent in self.ner_doc.ents:\n",
    "            if ent.label_ == \"PERSON\":\n",
    "                print(f\"{ent.text:<25} ‚Üí PERSON (spaCy)\")\n",
    "        print()  # Blank line for readability\n",
    "\n",
    "    # ----------------------------\n",
    "    # 2. Stanza-based validation\n",
    "    # ----------------------------\n",
    "    def validate_persons_with_stanza(self):\n",
    "        \"\"\"\n",
    "        Filters spaCy PERSON entities using Stanza POS (PROPN)\n",
    "        and prints valid and filtered entities.\n",
    "        \"\"\"\n",
    "        print(\"=== Stanza-Filtered PERSON Entities ===\")\n",
    "        filler_words = {'mmhm', 'uh', 'gotcha', 'okay', \"i've\", 'yeah', 'nope'}\n",
    "\n",
    "        for ent in self.ner_doc.ents:\n",
    "            if ent.label_ == \"PERSON\":\n",
    "                ent_start, ent_end = ent.start_char, ent.end_char\n",
    "                is_valid = False\n",
    "\n",
    "                # Check words in Stanza doc\n",
    "                for sentence in self.doc.sentences:\n",
    "                    for word in sentence.words:\n",
    "                        if word.start_char is not None and word.end_char is not None:\n",
    "                            # Check if Stanza word falls within spaCy entity span\n",
    "                            if ent_start <= word.start_char < ent_end:\n",
    "                                if word.upos == 'PROPN':\n",
    "                                    is_valid = True\n",
    "                                elif word.text.lower() in filler_words:\n",
    "                                    is_valid = False\n",
    "                                    break\n",
    "\n",
    "                # Print result\n",
    "                if is_valid:\n",
    "                    print(f\"{ent.text:<25} ‚Üí PERSON ‚úÖ (Valid)\")\n",
    "                else:\n",
    "                    print(f\"{ent.text:<25} ‚Üí PERSON ‚ùå (Filtered)\")\n",
    "\n",
    "        print()  # Blank line for readability\n",
    "\n",
    "    # ----------------------------\n",
    "    # 3. Title + Name Extraction (Stanza-only)\n",
    "    # ----------------------------\n",
    "    def extract_title_name_pairs(self):\n",
    "        \"\"\"\n",
    "        Finds patterns like 'Dr. Smith' using Stanza POS tagging.\n",
    "        \"\"\"\n",
    "        print(\"=== Title + Name Pairs (Stanza) ===\")\n",
    "        title_words = {'Dr.', 'Mr.', 'Ms.', 'Mrs.', 'Prof.', 'Professor'}\n",
    "\n",
    "        for sentence in self.doc.sentences:\n",
    "            words = sentence.words\n",
    "            for i in range(len(words) - 1):\n",
    "                curr_word = words[i]\n",
    "                next_word = words[i + 1]\n",
    "                if curr_word.text in title_words and next_word.upos == 'PROPN':\n",
    "                    title_name = f\"{curr_word.text} {next_word.text}\"\n",
    "                    print(title_name)\n",
    "        print()  # Blank line for readability\n",
    "\n",
    "    # ----------------------------\n",
    "    # 4. Mask PERSON entities in text\n",
    "    # ----------------------------\n",
    "    def replace_names(self):\n",
    "        \"\"\"\n",
    "        Replaces PERSON entities with 'NAME' in the text.\n",
    "        \"\"\"\n",
    "        masked_text = self.text\n",
    "        offset = 0\n",
    "        was_modified = False\n",
    "\n",
    "        for ent in self.ner_doc.ents:\n",
    "            if ent.label_ == 'PERSON':\n",
    "                start = ent.start_char + offset\n",
    "                end = ent.end_char + offset\n",
    "                original = masked_text[start:end]\n",
    "                masked_text = masked_text[:start] + \"NAME\" + masked_text[end:]\n",
    "                offset += len(\"NAME\") - len(original)\n",
    "                was_modified = True\n",
    "\n",
    "        print(\"ERRORS FOUND\" if was_modified else \"NO ERRORS\")\n",
    "        return masked_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca63a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "File: 058_712.txt\n",
      "  No colon spacing issues.\n",
      "  No likely speaker label typos.\n",
      "\n",
      "File: 058_713.txt\n",
      "  No colon spacing issues.\n",
      "  No likely speaker label typos.\n",
      "\n",
      "File: 055_684.txt\n",
      "  No colon spacing issues.\n",
      "  No likely speaker label typos.\n",
      "\n",
      "File: 055_685.txt\n",
      "  No colon spacing issues.\n",
      "  No likely speaker label typos.\n",
      "\n",
      "File: 057_707.txt\n",
      "  No colon spacing issues.\n",
      "  No likely speaker label typos.\n",
      "\n",
      "File: 057_706.txt\n",
      "  No colon spacing issues.\n",
      "  No likely speaker label typos.\n",
      "\n",
      "File: 056_696.txt\n",
      "  No colon spacing issues.\n",
      "  No likely speaker label typos.\n",
      "\n",
      "File: 056_697.txt\n",
      "  No colon spacing issues.\n",
      "  No likely speaker label typos.\n",
      "\n",
      "File: 056_691.txt\n",
      "  No colon spacing issues.\n",
      "  No likely speaker label typos.\n",
      "\n",
      "File: 056_690.txt\n",
      "  No colon spacing issues.\n",
      "  No likely speaker label typos.\n",
      "\n",
      "File: 058_708.txt\n",
      "  No colon spacing issues.\n",
      "  No likely speaker label typos.\n",
      "\n",
      "File: 058_709.txt\n",
      "  No colon spacing issues.\n",
      "  No likely speaker label typos.\n",
      "\n",
      "File: 056_700.txt\n",
      "  No colon spacing issues.\n",
      "  No likely speaker label typos.\n",
      "\n",
      "File: 055_689.txt\n",
      "  No colon spacing issues.\n",
      "  No likely speaker label typos.\n",
      "\n",
      "File: 055_688.txt\n",
      "  No colon spacing issues.\n",
      "  No likely speaker label typos.\n",
      "\n",
      "File: 058_715.txt\n",
      "  No colon spacing issues.\n",
      "  Fuzzy Matches (Likely Misspellings):\n",
      "    - Interviewer 31 ‚Üí Did you mean: Interviewer?\n",
      "\n",
      "File: 058_714.txt\n",
      "  No colon spacing issues.\n",
      "  Fuzzy Matches (Likely Misspellings):\n",
      "    - Interviewer 28 ‚Üí Did you mean: Interviewer?\n",
      "\n",
      "File: 055_683.txt\n",
      "  No colon spacing issues.\n",
      "  No likely speaker label typos.\n",
      "\n",
      "File: 055_682.txt\n",
      "  No colon spacing issues.\n",
      "  No likely speaker label typos.\n",
      "\n",
      "File: 056_695.txt\n",
      "  No colon spacing issues.\n",
      "  No likely speaker label typos.\n",
      "\n",
      "File: 056_694.txt\n",
      "  No colon spacing issues.\n",
      "  No likely speaker label typos.\n",
      "\n",
      "File: 058_711.txt\n",
      "  No colon spacing issues.\n",
      "  No likely speaker label typos.\n",
      "\n",
      "File: 058_710.txt\n",
      "  No colon spacing issues.\n",
      "  No likely speaker label typos.\n",
      "\n",
      "File: 055_687.txt\n",
      "  No colon spacing issues.\n",
      "  No likely speaker label typos.\n",
      "\n",
      "File: 055_686.txt\n",
      "  No colon spacing issues.\n",
      "  No likely speaker label typos.\n",
      "\n",
      "File: 057_704.txt\n",
      "  No colon spacing issues.\n",
      "  No likely speaker label typos.\n",
      "\n",
      "File: 057_705.txt\n",
      "  No colon spacing issues.\n",
      "  No likely speaker label typos.\n",
      "\n",
      "File: 055_680.txt\n",
      "  No colon spacing issues.\n",
      "  No likely speaker label typos.\n",
      "\n",
      "File: 055_681.txt\n",
      "  No colon spacing issues.\n",
      "  No likely speaker label typos.\n",
      "\n",
      "File: 057_703.txt\n",
      "  No colon spacing issues.\n",
      "  No likely speaker label typos.\n",
      "\n",
      "File: 057_702.txt\n",
      "  No colon spacing issues.\n",
      "  No likely speaker label typos.\n",
      "\n",
      "File: 056_698.txt\n",
      "  No colon spacing issues.\n",
      "  No likely speaker label typos.\n",
      "\n",
      "File: 056_699.txt\n",
      "  No colon spacing issues.\n",
      "  No likely speaker label typos.\n",
      "\n",
      "File: 056_692.txt\n",
      "  No colon spacing issues.\n",
      "  No likely speaker label typos.\n",
      "\n",
      "File: 056_693.txt\n",
      "  No colon spacing issues.\n",
      "  No likely speaker label typos.\n",
      "\n",
      "File: 055_678.txt\n",
      "  No colon spacing issues.\n",
      "  No likely speaker label typos.\n",
      "\n",
      "File: 055_679.txt\n",
      "  No colon spacing issues.\n",
      "  No likely speaker label typos.\n"
     ]
    }
   ],
   "source": [
    "# this starts the other error detector functions, other than names:\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "from difflib import get_close_matches\n",
    "import re\n",
    "\n",
    "from utils.document import SPEAKER_PAIRS\n",
    "from utils.document import SPEAKERS\n",
    "\n",
    "# Error Detection Functions\n",
    "\n",
    "# Finds speaker labels with colon spacing variation\n",
    "def find_colon_spacing_issues(text):\n",
    "    pattern = re.compile(r'\\b(?:' + '|'.join(re.escape(s) for s in SPEAKERS) + r')\\s+:')\n",
    "    return pattern.findall(text)\n",
    "\n",
    "# Finds speaker labels followed by punctuation other than :\n",
    "def find_bad_punctuation_after_labels(text):\n",
    "    pattern = re.compile(r'\\b(?:' + '|'.join(re.escape(s) for s in SPEAKERS) + r')[^\\s:]')\n",
    "    return pattern.findall(text)\n",
    "\n",
    "# Finds misspellings of speaker labels \n",
    "def find_spelling_variants(text, threshold=0.8):\n",
    "    pattern = re.compile(r'^([A-Z][a-zA-Z0-9_ ]{1,30})(?=\\s*:\\s*)', re.MULTILINE)\n",
    "    candidates = pattern.findall(text)\n",
    "\n",
    "    fuzzy_hits = {}\n",
    "    for cand in candidates:\n",
    "        matches = get_close_matches(cand, SPEAKERS, n=1, cutoff=threshold)\n",
    "        if matches and matches[0] != cand:\n",
    "            fuzzy_hits[cand] = matches[0]\n",
    "    return fuzzy_hits\n",
    "\n",
    "\n",
    "# --- Path to your folder of review files\n",
    "project_root = Path.cwd().parent\n",
    "review_dir = project_root / \"data\" / \"s1055-1058\" / \"REVIEW\"\n",
    "\n",
    "# --- Loop through all .txt.json files\n",
    "for file_path in review_dir.glob(\"*.txt.json\"):\n",
    "    try:\n",
    "        doc = Document(str(file_path))\n",
    "        text = doc.full_content\n",
    "\n",
    "        spacing_issues = find_colon_spacing_issues(text)\n",
    "        fuzzy_issues = find_spelling_variants(text)\n",
    "\n",
    "        # Print results for this file\n",
    "        print(f\"\\nFile: {doc.name}\")\n",
    "\n",
    "        if spacing_issues:\n",
    "            print(\"  Spacing Issues (Extra Space Before Colon):\")\n",
    "            for issue in spacing_issues:\n",
    "                print(f\"    - {issue}\")\n",
    "        else:\n",
    "            print(\"  No colon spacing issues.\")\n",
    "\n",
    "        if fuzzy_issues:\n",
    "            print(\"  Fuzzy Matches (Likely Misspellings):\")\n",
    "            for wrong, correct in fuzzy_issues.items():\n",
    "                print(f\"    - {wrong} ‚Üí Did you mean: {correct}?\")\n",
    "        else:\n",
    "            print(\"  No likely speaker label typos.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing {file_path.name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b667bea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022_352.txt ‚Äî No multi-speaker lines found.\n",
      "2022_353.txt ‚Äî No multi-speaker lines found.\n",
      "3001_079.txt ‚Äî No multi-speaker lines found.\n",
      "3001_078.txt ‚Äî No multi-speaker lines found.\n",
      "062_740.txt ‚Äî No multi-speaker lines found.\n",
      "062_741.txt ‚Äî No multi-speaker lines found.\n",
      "2026_386.txt ‚Äî No multi-speaker lines found.\n",
      "2026_387.txt ‚Äî No multi-speaker lines found.\n",
      "2024_370.txt ‚Äî No multi-speaker lines found.\n",
      "2024_371.txt ‚Äî No multi-speaker lines found.\n",
      "3001_096.txt ‚Äî No multi-speaker lines found.\n",
      "3001_097.txt ‚Äî No multi-speaker lines found.\n",
      "2022_345.txt ‚Äî No multi-speaker lines found.\n",
      "2022_344.txt ‚Äî No multi-speaker lines found.\n",
      "2022_332.txt ‚Äî No multi-speaker lines found.\n",
      "2022_333.txt ‚Äî No multi-speaker lines found.\n",
      "2026_391.txt ‚Äî No multi-speaker lines found.\n",
      "2026_390.txt ‚Äî No multi-speaker lines found.\n",
      "2024_367.txt ‚Äî No multi-speaker lines found.\n",
      "2024_366.txt ‚Äî No multi-speaker lines found.\n",
      "3001_081.txt ‚Äî No multi-speaker lines found.\n",
      "3001_080.txt ‚Äî No multi-speaker lines found.\n",
      "2022_338.txt ‚Äî No multi-speaker lines found.\n",
      "2022_339.txt ‚Äî No multi-speaker lines found.\n",
      "2023_359.txt ‚Äî No multi-speaker lines found.\n",
      "2023_358.txt ‚Äî No multi-speaker lines found.\n",
      "2022_342.txt ‚Äî No multi-speaker lines found.\n",
      "2022_343.txt ‚Äî No multi-speaker lines found.\n",
      "2022_335.txt ‚Äî No multi-speaker lines found.\n",
      "2022_334.txt ‚Äî No multi-speaker lines found.\n",
      "2026_396.txt ‚Äî No multi-speaker lines found.\n",
      "2026_397.txt ‚Äî No multi-speaker lines found.\n",
      "2022_348.txt ‚Äî No multi-speaker lines found.\n",
      "2022_349.txt ‚Äî No multi-speaker lines found.\n",
      "2024_360.txt ‚Äî No multi-speaker lines found.\n",
      "2024_361.txt ‚Äî No multi-speaker lines found.\n",
      "3001_086.txt ‚Äî No multi-speaker lines found.\n",
      "3001_087.txt ‚Äî No multi-speaker lines found.\n",
      "2022_355.txt ‚Äî No multi-speaker lines found.\n",
      "2022_354.txt ‚Äî No multi-speaker lines found.\n",
      "062_746.txt ‚Äî No multi-speaker lines found.\n",
      "2026_381.txt ‚Äî No multi-speaker lines found.\n",
      "2024_377.txt ‚Äî No multi-speaker lines found.\n",
      "2024_376.txt ‚Äî No multi-speaker lines found.\n",
      "2022_328.txt ‚Äî No multi-speaker lines found.\n",
      "2022_329.txt ‚Äî No multi-speaker lines found.\n",
      "3001_091.txt ‚Äî No multi-speaker lines found.\n",
      "3001_090.txt ‚Äî No multi-speaker lines found.\n",
      "2024_364.txt ‚Äî No multi-speaker lines found.\n",
      "2024_365.txt ‚Äî No multi-speaker lines found.\n",
      "3001_082.txt ‚Äî No multi-speaker lines found.\n",
      "3001_083.txt ‚Äî No multi-speaker lines found.\n",
      "2026_398.txt ‚Äî No multi-speaker lines found.\n",
      "2022_346.txt ‚Äî No multi-speaker lines found.\n",
      "2022_347.txt ‚Äî No multi-speaker lines found.\n",
      "2022_331.txt ‚Äî No multi-speaker lines found.\n",
      "2022_330.txt ‚Äî No multi-speaker lines found.\n",
      "2026_392.txt ‚Äî No multi-speaker lines found.\n",
      "2026_393.txt ‚Äî No multi-speaker lines found.\n",
      "3001_088.txt ‚Äî No multi-speaker lines found.\n",
      "3001_089.txt ‚Äî No multi-speaker lines found.\n",
      "2024_373.txt ‚Äî No multi-speaker lines found.\n",
      "2024_372.txt ‚Äî No multi-speaker lines found.\n",
      "3001_095.txt ‚Äî No multi-speaker lines found.\n",
      "3001_094.txt ‚Äî No multi-speaker lines found.\n",
      "2022_351.txt ‚Äî No multi-speaker lines found.\n",
      "2022_350.txt ‚Äî No multi-speaker lines found.\n",
      "2024_379.txt ‚Äî No multi-speaker lines found.\n",
      "2024_378.txt ‚Äî No multi-speaker lines found.\n",
      "062_743.txt ‚Äî No multi-speaker lines found.\n",
      "062_742.txt ‚Äî No multi-speaker lines found.\n",
      "2026_385.txt ‚Äî No multi-speaker lines found.\n",
      "2026_384.txt ‚Äî No multi-speaker lines found.\n",
      "2022_326.txt ‚Äî No multi-speaker lines found.\n",
      "2022_327.txt ‚Äî No multi-speaker lines found.\n",
      "3001_077.txt ‚Äî No multi-speaker lines found.\n",
      "3001_076.txt ‚Äî No multi-speaker lines found.\n",
      "2024_374.txt ‚Äî No multi-speaker lines found.\n",
      "2024_375.txt ‚Äî No multi-speaker lines found.\n",
      "062_739.txt ‚Äî No multi-speaker lines found.\n",
      "2026_388.txt ‚Äî No multi-speaker lines found.\n",
      "2026_389.txt ‚Äî No multi-speaker lines found.\n",
      "3001_092.txt ‚Äî No multi-speaker lines found.\n",
      "3001_093.txt ‚Äî No multi-speaker lines found.\n",
      "2022_356.txt ‚Äî No multi-speaker lines found.\n",
      "062_744.txt ‚Äî No multi-speaker lines found.\n",
      "062_745.txt ‚Äî No multi-speaker lines found.\n",
      "2026_382.txt ‚Äî No multi-speaker lines found.\n",
      "2026_383.txt ‚Äî No multi-speaker lines found.\n",
      "2024_363.txt ‚Äî No multi-speaker lines found.\n",
      "2024_362.txt ‚Äî No multi-speaker lines found.\n",
      "3001_085.txt ‚Äî No multi-speaker lines found.\n",
      "3001_084.txt ‚Äî No multi-speaker lines found.\n",
      "2024_369.txt ‚Äî No multi-speaker lines found.\n",
      "2024_368.txt ‚Äî No multi-speaker lines found.\n",
      "2022_341.txt ‚Äî No multi-speaker lines found.\n",
      "2022_340.txt ‚Äî No multi-speaker lines found.\n",
      "2022_336.txt ‚Äî No multi-speaker lines found.\n",
      "2022_337.txt ‚Äî No multi-speaker lines found.\n",
      "2026_395.txt ‚Äî No multi-speaker lines found.\n",
      "2026_394.txt ‚Äî No multi-speaker lines found.\n",
      "2023_357.txt ‚Äî No multi-speaker lines found.\n"
     ]
    }
   ],
   "source": [
    "# --- Detection function ---\n",
    "def find_multi_speaker_lines(text, SPEAKERS):\n",
    "    speaker_pattern = r'\\b(?:' + '|'.join(re.escape(s) for s in SPEAKERS) + r')\\s*:'\n",
    "    pattern = re.compile(speaker_pattern)\n",
    "    multi_speaker_lines = []\n",
    "    for i, line in enumerate(text.splitlines()):\n",
    "        matches = pattern.findall(line)\n",
    "        if len(matches) > 1:\n",
    "            multi_speaker_lines.append((i + 1, line.strip(), matches))\n",
    "    return multi_speaker_lines\n",
    "\n",
    "# --- Fixer function ---\n",
    "'''def split_multi_speaker_line(line, speaker_set):\n",
    "    speaker_pattern = r'(?<!^)(\\b(?:' + '|'.join(re.escape(s) for s in speaker_set) + r')\\s*:)'\n",
    "    return re.sub(speaker_pattern, r'\\n\\1', line)'''\n",
    "\n",
    "# --- Folder setup ---\n",
    "project_root = Path.cwd().parent\n",
    "review_dir = project_root / \"data\" / \"s1062_s2022-26_s3076-97\" / \"REVIEW\"\n",
    "\n",
    "# --- Loop through files ---\n",
    "for file_path in review_dir.glob(\"*.txt.json\"):\n",
    "    try:\n",
    "        doc = Document(str(file_path))\n",
    "        original_text = doc.full_content\n",
    "\n",
    "        multi_speaker_lines = find_multi_speaker_lines(original_text, SPEAKERS)\n",
    "\n",
    "        if not multi_speaker_lines:\n",
    "            print(f\"{doc.name} ‚Äî No multi-speaker lines found.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n{doc.name} ‚Äî {len(multi_speaker_lines)} multi-speaker lines found:\")\n",
    "        for lineno, line, matches in multi_speaker_lines:\n",
    "            print(f\"  Line {lineno}: {matches}\")\n",
    "            print(f\"    {line}\")\n",
    "\n",
    "        '''# --- Fix and rebuild text ---\n",
    "        fixed_lines = []\n",
    "        for line in original_text.splitlines():\n",
    "            fixed = split_multi_speaker_line(line, SPEAKERS)\n",
    "            fixed_lines.extend(fixed.splitlines())\n",
    "\n",
    "        fixed_text = '\\n'.join(fixed_lines)\n",
    "\n",
    "        # --- Save fixed version ---\n",
    "        masked_dir = project_root / \"data\" / \"s1055-1058\" / \"REVIEW_fixed\"\n",
    "        masked_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        fixed_filename = Path(doc.name).stem + \"_split.txt.json\"\n",
    "        output_path = masked_dir / fixed_filename\n",
    "\n",
    "        # Save using the same JSON structure style\n",
    "        output_json = {\n",
    "            \"version\": \"1.0\",\n",
    "            \"data\": {\n",
    "                \"document\": {\n",
    "                    \"name\": fixed_filename\n",
    "                },\n",
    "                \"content\": fixed_text\n",
    "            }\n",
    "        }\n",
    "\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            import json\n",
    "            json.dump(output_json, f, indent=2)\n",
    "\n",
    "        print(f\"Fixed version saved to: {output_path.name}\")'''\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing {file_path.name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad08d75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bde3b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' # Create new filename\n",
    "original_filename = Path(doc.name).stem  # removes .txt.json\n",
    "new_filename = original_filename + \"_masked.txt.json\"\n",
    "\n",
    "# Define output path\n",
    "masked_dir = Path(\"masked\")\n",
    "masked_dir.mkdir(parents=True, exist_ok=True)\n",
    "output_path = masked_dir / new_filename\n",
    "\n",
    "# Create json structure\n",
    "output_json = {\n",
    "    \"version\": \"1.0\",\n",
    "    \"data\": {\n",
    "        \"document\": {\n",
    "            \"name\": new_filename\n",
    "        },\n",
    "        \"content\": masked_text\n",
    "    }\n",
    "}\n",
    "\n",
    "# Write to file\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(output_json, f, indent=2)\n",
    "\n",
    "print(f\"\\n Masked output saved to: {output_path.resolve()}\")'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
